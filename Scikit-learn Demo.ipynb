{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scikit-Learn for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn is a python library for machine learning. Some things you can do with [scikit-learn](http://scikit-learn.org/stable/)\n",
    "<img src=\"scikit-learn.png\" width=\"800\">\n",
    "\n",
    "In this demo we'll see:\n",
    "1. How to load data from different sources.\n",
    "1. How to do apply some data preprocessing using [scikit-learn](http://scikit-learn.org/stable/) and [Pandas](https://pandas.pydata.org/), which is a collection of python tools for data analysis.\n",
    "2. How to use scikit-learn for cross-validation\n",
    "2. How to run all the algorithms we've seen in class (decision tree, perceptron, SVM) with scikit-learn\n",
    "\n",
    "\n",
    "*Note*: The tutorials here are not an exhaustive representation of what we can do with these tools. They are meant to give you an starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Installed Datsets\n",
    "\n",
    "Scikit learn has some [pre-installed datasets](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets). We're going to check out the \"Iris\" dataset. More information on the Iris dataset is available here: \n",
    "1. [UCI Machine learning Repository](https://archive.ics.uci.edu/ml/datasets/iris)\n",
    "2. [Wikipedia article about the dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set)\n",
    "\n",
    "Let us first load the dataset. (To run the any of the code cells, you click on it and press Shift+Enter.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us quickly see what the dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.keys())\n",
    "#You can check out the description of the dataset using the following command\n",
    "#print(iris.DESCR)\n",
    "\n",
    "n_samples, n_features = iris.data.shape\n",
    "print('Number of samples:', n_samples)\n",
    "print('Number of features:', n_features)\n",
    "# the sepal length, sepal width, petal length and petal width of the first sample (first flower)\n",
    "print('Features:', iris.feature_names)\n",
    "print(iris.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the features:\", iris.data.shape)\n",
    "print(\"Shape of the labels:\", iris.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of the labels\n",
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a scatter plot of the data to get an idea of how it looks. For this we'll use python library, Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# A scatter plot of the 3rd feature against the 0th feature.\n",
    "x_index = 3\n",
    "y_index = 0\n",
    "\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "for label, color in zip(range(len(iris.target_names)), colors):\n",
    "    plt.scatter(iris.data[iris.target==label, x_index], \n",
    "                iris.data[iris.target==label, y_index],\n",
    "                label=iris.target_names[label],\n",
    "                c=color)\n",
    "\n",
    "plt.xlabel(iris.feature_names[x_index])\n",
    "plt.ylabel(iris.feature_names[y_index])\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading your own dataset\n",
    "\n",
    "There are different places where you can find data sets like Kaggle, [the UCI repository](https://archive.ics.uci.edu/ml/datasets.html), etc.\n",
    "\n",
    "Here, we'll be using the Titanic Passenger Survival Data Set. The description can be found here: https://www.kaggle.com/c/titanic/data. Briefly, the goal is to predict whether a passenger survived the Titanic disaster or not.\n",
    "\n",
    "We have already included the dataset in this git repository in the file `titanic.csv`. Let us load it from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#loading the datset from .csv file\n",
    "titanic = pd.read_csv(os.path.join('data', 'titanic.csv'))\n",
    "\n",
    "#print columns in the table\n",
    "print(\"Columns in titanic.csv: \" + str(list(titanic.columns)))\n",
    "\n",
    "#labels are stored in column named 'survived'. Let us save it in a variable called labels.\n",
    "labels = titanic.survived.values\n",
    "\n",
    "#fetch the columns that we'll be using for our models\n",
    "data = titanic[['pclass', 'sex', 'sibsp', 'parch', 'embarked']]\n",
    "print(\"Data shape (rows, columns) =  \", data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start with the machine learning, let's see the survival rate based on the gender of the passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split the data by male/female.\n",
    "df_male = titanic.survived[titanic.sex == 'male'].value_counts().sort_index()\n",
    "df_female = titanic.survived[titanic.sex == 'female'].value_counts().sort_index()\n",
    "\n",
    "print(df_male)\n",
    "print(df_female)\n",
    "\n",
    "# Use matplotlib again for plotting \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "label_values = [0,1]\n",
    "plt.bar([l - 0.15 for l in label_values], df_male, 0.3, label = 'Male', alpha=0.55)\n",
    "plt.bar([l + 0.15 for l in label_values], df_female, 0.3,color='#FA2379', label = 'Female', alpha=0.55)\n",
    "ax.set_xticks([0,1])\n",
    "ax.set_xticklabels(['Not survived', 'Survived'])\n",
    "plt.title(\"Who Survived? with respect to Gender, (raw value counts) \"); plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Scikit-learn with real world data: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at some techniques for transforming the features now.\n",
    "\n",
    "### Binning\n",
    "Binning is a technique which is used to convert continuous values to discrete values for the ease of classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at ages and fares\n",
    "print('age')\n",
    "print(titanic['age'].head(5))\n",
    "print()\n",
    "print('fare')\n",
    "print(titanic['fare'].head(5))\n",
    "print()\n",
    "\n",
    "#lets look at min, max, median and mean for age and fare. This will help us in deciding the bins\n",
    "print('property | Age | Fare')\n",
    "print('min', titanic['age'].min(), titanic['fare'].min())\n",
    "print('max', titanic['age'].max(), titanic['fare'].max())\n",
    "print('median', titanic['age'].median(), titanic['fare'].median())\n",
    "print('mean', titanic['age'].mean(), titanic['fare'].mean())\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create bins for age and fare\n",
    "bins_age = [0, 25, 60,  100]\n",
    "bins_fare = [0, 25, 100, 1000]\n",
    "\n",
    "age_groups = ['young', 'adult', 'senior']\n",
    "fare_groups = ['low', 'medium', 'high']\n",
    "\n",
    "data.loc[:, 'age'] = pd.cut(titanic.loc[:, 'age'], bins_age, labels=age_groups)\n",
    "data.loc[:, 'fare'] = pd.cut(titanic.loc[:, 'fare'], bins_fare, labels=fare_groups)\n",
    "\n",
    "#remove original columns\n",
    "# del data['age']\n",
    "# del data['fare']\n",
    "\n",
    "data.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Categorical values to Numerical values\n",
    "\n",
    "Here we will be converting categorical values like Male/Female to numerical values like 1/2 for easier processing.\n",
    "\n",
    "Here, we'll be using 2 different techniques:\n",
    "- LabelEncoder: This is used when none of the values for this column are missing.\n",
    "- factorize: This is used when there are missing values. The missing value is assigned the value of -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "#let's check the values before we change categorical values to numbers.\n",
    "print(data.head(5))\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['sex'] = le.fit_transform(data['sex'])\n",
    "\n",
    "#select only those columns that are categorical\n",
    "for column in data.select_dtypes(include=['object', 'category']).columns:\n",
    "    data[column] = data[column].factorize()[0]\n",
    "\n",
    "#after processing\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values\n",
    "\n",
    "There are different ways to handle missing values. Here we're using `fillna()` from pandas since we had categorical values. You could also use `Imputer` from scikit-learn to fill with mean, median etc. See [the documentation](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html) to learn more about it.\n",
    "\n",
    "In either case, we need to replace -1 with `np.nan` (i.e. not-a-number). This is needed because `factorize()` from above encodes missing values as -1.\n",
    "\n",
    "Not that, `Imputer` returns an `ndarray1`, whereas `fillna` gives us a `dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "import numpy as np \n",
    "\n",
    "data = data.replace(-1, np.nan)\n",
    "\n",
    "#try this\n",
    "# imp = Imputer(strategy=\"median\") # can supply different strategies\n",
    "# imp.fit(data)\n",
    "# data = imp.transform(data)\n",
    "\n",
    "data = data.fillna(data.median()) # can try with sum, mean etc.\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Categorical Values to Boolean Values (Binary Values)\n",
    "We will be using one-hot-encoding to convert categorical features to binary features.\n",
    "\n",
    "#### What is one-hot-encoding?\n",
    "\n",
    "When a column has categorical values, it is hard for the machine learning algorithm to train  upon. To make it more suitable for the ML algorithms, we convert each category for that column to a boolean column. Only one of the columns can take a value of one for a single sample. Hence, it is called as one hot encoding. \n",
    "\n",
    "(eg) Suppose you have ‘flower’ feature which can take values ‘daffodil’, ‘lily’, and ‘rose’. One hot encoding converts ‘flower’ feature to three features, ‘is_daffodil’, ‘is_lily’, and ‘is_rose’ which all are binary.\n",
    "\n",
    "We will be using pandas' get_dummies() function which is equivalent to scikit-learn [OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder).\n",
    "\n",
    "Refer: https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert all categorical columns to boolean columns\n",
    "# data = pd.get_dummies(data)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc_data = enc.fit_transform(data).toarray()\n",
    "\n",
    "# you can also select the columns to convert. I don't know why you would do that though!\n",
    "# data = pd.get_dummies(data, columns=[ 'sex', 'embarked', 'age', 'fare'])\n",
    "print('columns:', enc_data.shape[1])\n",
    "print('rows:', enc_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "processed_age_data = preprocessing.scale(data['age'])\n",
    "\n",
    "print(\"The mean of the original age data is\", data['age'].mean(axis=0))\n",
    "print(\"The std  of the original age data is\", data['age'].std(axis=0))\n",
    "print()\n",
    "\n",
    "print(\"The mean of the transformed age data is\", processed_age_data.mean(axis=0))\n",
    "print(\"The std  of the transformed age data is\", processed_age_data.std(axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Test-Train Split\n",
    "Here we'll create train and test splits to train and test our algorithm.\n",
    "\n",
    "We'll keep two sets: \n",
    "1. without OneHotEncoding (we will call this split A) and \n",
    "2. with OneHotEncoding (split B).\n",
    "\n",
    "We will compare the results of the two for each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits without encoded values. We will call this split 'A'\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, random_state=0)\n",
    "\n",
    "# splits with encoded values. We will call this split 'B'\n",
    "enc_train_data, enc_test_data, enc_train_labels, enc_test_labels = train_test_split(enc_data, labels, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly predicting a label\n",
    "\n",
    "Here we'll see a classifier that predicts labels at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "clf = DummyClassifier(\"uniform\")\n",
    "clf.fit(train_data, train_labels)\n",
    "print(\"Prediction accuracy:\", clf.score(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the accuracy from chance is 50%. Let's try a better classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common Label\n",
    "\n",
    "Scikit learn calls this a \"dummy\" classifier. Easy \"baseline\" for learning. Just labels all the instances with the most common/frequent label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DummyClassifier('most_frequent')\n",
    "clf.fit(train_data, train_labels)\n",
    "print(\"Prediction accuracy:\", clf.score(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms we have seen in class\n",
    "\n",
    "### SVM\n",
    "\n",
    "The support vector machine is called `LinearSVC` in scikit learn. (SVC stands for Support Vector Classification). You can read more about it [here](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC(random_state=4000)\n",
    "\n",
    "#train on split A\n",
    "clf.fit(train_data, train_labels)\n",
    "\n",
    "# Test on split A\n",
    "print(clf.coef_)\n",
    "print(clf.intercept_)\n",
    "print()\n",
    "print(\"Prediction accuracy:\", clf.score(test_data, test_labels))\n",
    "print()\n",
    "\n",
    "#train on split B\n",
    "clf.fit(enc_train_data, enc_train_labels)\n",
    "\n",
    "# Test on split B\n",
    "print(clf.coef_)\n",
    "print(clf.intercept_)\n",
    "print()\n",
    "print(\"Prediction accuracy:\", clf.score(enc_test_data, enc_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validating SVM\n",
    "\n",
    "We will be cross validating for parameter C. As you might remember from class, 'C' is the penalty parameter of the error formula. \n",
    "\n",
    "[Documentation](http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'C': [1e-3, 1e-2, 1e-2, 1, 10, 100, 1000, 5000]}]\n",
    "\n",
    "print(\"# Tuning hyper-parameters for accuracy\")\n",
    "print()\n",
    "\n",
    "clf = GridSearchCV(LinearSVC(), tuned_parameters, cv=5, scoring='accuracy')\n",
    "clf.fit(train_data, train_labels)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "#train and test by using best parameters.\n",
    "clf = LinearSVC(random_state=4000, C=clf.best_params_['C'])\n",
    "clf.fit(train_data, train_labels)\n",
    "\n",
    "print(clf.coef_)\n",
    "print(clf.intercept_)\n",
    "print()\n",
    "print(\"Prediction accuracy:\", clf.score(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "clf = Perceptron(random_state=2000)\n",
    "\n",
    "#train and test on split A\n",
    "clf.fit(train_data, train_labels)\n",
    "\n",
    "print(clf.coef_)\n",
    "print(clf.intercept_)\n",
    "print()\n",
    "print(\"Prediction accuracy:\", clf.score(test_data, test_labels))\n",
    "\n",
    "#train and test on split B\n",
    "clf.fit(enc_train_data, enc_train_labels)\n",
    "\n",
    "print(clf.coef_)\n",
    "print(clf.intercept_)\n",
    "print()\n",
    "print(\"Prediction accuracy:\", clf.score(enc_test_data, enc_test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/tree.html\n",
    "#for visualization\n",
    "\n",
    "import graphviz \n",
    "from sklearn import tree\n",
    "\n",
    "#for windows, uncomment the following and replace with path of you local Graphviz.38/bin. Include path of bin and not python lib\n",
    "# os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini')\n",
    "clf.fit(train_data, train_labels)\n",
    "DecisionTreeClassifier()\n",
    "\n",
    "print(\"Accuracy\", clf.score(test_data, test_labels))\n",
    "print(\"The depth of this tree is\", clf.tree_.max_depth)\n",
    "\n",
    "graph = graphviz.Source(tree.export_graphviz(clf, out_file=None)) \n",
    "graph.render('graphs/dt_gini')\n",
    "\n",
    "clf.fit(enc_train_data, enc_train_labels)\n",
    "DecisionTreeClassifier()\n",
    "\n",
    "print(\"Accuracy\", clf.score(enc_test_data, enc_test_labels))\n",
    "print(\"The depth of this tree is\", clf.tree_.max_depth)\n",
    "\n",
    "\n",
    "graph = graphviz.Source(tree.export_graphviz(clf, out_file=None)) \n",
    "graph.render('graphs/dt_gini_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try using a different criterion to build the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion='entropy')\n",
    "clf.fit(train_data, train_labels)\n",
    "DecisionTreeClassifier()\n",
    "\n",
    "print(\"Accuracy\", clf.score(test_data, test_labels))\n",
    "print(\"The depth of this tree is\", clf.tree_.max_depth)\n",
    "\n",
    "graph = graphviz.Source(tree.export_graphviz(clf, out_file=None)) \n",
    "graph.render('graphs/dt_entropy')\n",
    "\n",
    "clf.fit(enc_train_data, enc_train_labels)\n",
    "DecisionTreeClassifier()\n",
    "\n",
    "print(\"Accuracy\", clf.score(enc_test_data, enc_test_labels))\n",
    "print(\"The depth of this tree is\", clf.tree_.max_depth)\n",
    "\n",
    "\n",
    "graph = graphviz.Source(tree.export_graphviz(clf, out_file=None)) \n",
    "graph.render('graphs/dt_entropy_enc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try limiting the depth and visualize our tree...\n",
    "\n",
    "http://scikit-learn.org/stable/modules/tree.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=2)\n",
    "clf.fit(train_data, train_labels)\n",
    "DecisionTreeClassifier()\n",
    "\n",
    "print(\"Accuracy\", clf.score(test_data, test_labels))\n",
    "print(\"The depth of this tree is\", clf.tree_.max_depth)\n",
    "\n",
    "graph = graphviz.Source(tree.export_graphviz(clf, out_file=None)) \n",
    "graph.render('graphs/dt_gini_limit')\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.tree_.__getstate__()\n",
    "clf.fit(enc_train_data, enc_train_labels)\n",
    "DecisionTreeClassifier()\n",
    "\n",
    "print(\"Accuracy\", clf.score(enc_test_data, enc_test_labels))\n",
    "print(\"The depth of this tree is\", clf.tree_.max_depth)\n",
    "\n",
    "graph = graphviz.Source(tree.export_graphviz(clf, out_file=None)) \n",
    "graph.render('graphs/dt_gini_limit_enc')\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Here we'll be training a random forest. Try modifying the parameters and see how it affects the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=6, random_state=0, n_estimators=35, criterion='gini')\n",
    "clf.fit(train_data, train_labels)\n",
    "print(clf.feature_importances_)\n",
    "print()\n",
    "print(\"Prediction accuracy:\", clf.score(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other algorithms\n",
    "\n",
    "- Logistic Regression\n",
    "- Naive Bayes\n",
    "- Adaboost\n",
    "- K Nearest Neighbors\n",
    "- Multiclass classifiers\n",
    "- SVM with Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Try you hands at logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "#train and test on split A\n",
    "clf.fit(train_data, train_labels)\n",
    "print('Prediction Accuracy:', clf.score(test_data, test_labels))\n",
    "\n",
    "#train and test on split B\n",
    "clf.fit(enc_train_data, enc_train_labels)\n",
    "print('Prediction Accuracy:', clf.score(enc_test_data, enc_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=75)\n",
    "\n",
    "#train and test on split A\n",
    "clf.fit(train_data, train_labels)\n",
    "print('Prediction Accuracy:', clf.score(test_data, test_labels))\n",
    "\n",
    "#train and test on split B\n",
    "clf.fit(enc_train_data, enc_train_labels)\n",
    "print('Prediction Accuracy:', clf.score(enc_test_data, enc_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "#train and test on split A\n",
    "clf.fit(train_data, train_labels)\n",
    "print('Prediction Accuracy:', clf.score(test_data, test_labels))\n",
    "\n",
    "#train and test on split B\n",
    "clf.fit(enc_train_data, enc_train_labels)\n",
    "print('Prediction Accuracy:', clf.score(enc_test_data, enc_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class classification\n",
    "\n",
    "We come back to iris dataset and use some techniques of multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_train_data, iris_test_data, iris_train_labels, iris_test_labels = train_test_split(iris.data, iris.target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-vs-all\n",
    "clf = LinearSVC(random_state=4000, multi_class='ovr')\n",
    "\n",
    "clf.fit(iris_train_data, iris_train_labels)\n",
    "\n",
    "print(\"Prediction accuracy:\", clf.score(iris_test_data, iris_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-vs-one\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(random_state=4000)\n",
    "\n",
    "clf.fit(iris_train_data, iris_train_labels)\n",
    "\n",
    "print(\"Prediction accuracy:\", clf.score(iris_test_data, iris_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi-class\n",
    "clf = LinearSVC(random_state=4000, multi_class='crammer_singer')\n",
    "\n",
    "clf.fit(iris_train_data, iris_train_labels)\n",
    "\n",
    "print(\"Prediction accuracy:\", clf.score(iris_test_data, iris_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion='gini')\n",
    "clf.fit(iris_train_data, iris_train_labels)\n",
    "DecisionTreeClassifier()\n",
    "\n",
    "print(\"Accuracy\", clf.score(iris_test_data, iris_test_labels))\n",
    "print(\"The depth of this tree is\", clf.tree_.max_depth)\n",
    "\n",
    "\n",
    "graph = graphviz.Source(tree.export_graphviz(clf, out_file=None)) \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links!\n",
    "- Scikit learn official page: http://scikit-learn.org/stable/index.html\n",
    "\n",
    "- Pandas official page: http://pandas.pydata.org/pandas-docs/stable/index.html\n",
    "\n",
    "- lots of scikit demos: https://github.com/amueller/scipy-2016-sklearn/tree/master/notebooks\n",
    "\n",
    "- svm documentation: http://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "- decision tree documentation: http://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "- perceptron documentation: http://scikit-learn.org/stable/modules/linear_model.html#perceptron\n",
    "\n",
    "- graphing trees: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
    "\n",
    "- cross-validation parameter search documentation: http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py\n",
    "- Parts of this tutorial have been based on this tutorial: https://github.com/amueller/scipy-2016-sklearn/blob/master/notebooks/10%20Case%20Study%20-%20Titanic%20Survival.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
