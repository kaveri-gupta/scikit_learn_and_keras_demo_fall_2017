{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Setting up the Data\n",
    "\n",
    "Adapted from https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py\n",
    "\n",
    "\n",
    "Note: use Shift+Enter to run the codeblocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure matplotlib figure size and make inline\n",
    "matplotlib.rcParams['figure.figsize'] = [10, 6]\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split our data into a training & testing set\n",
    "\n",
    "`x_train` & `x_test` are the examples, and `y_train` & `y_test` are the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test  = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')\n",
    "\n",
    "# normalize the pixel values to be in the range (0, 1)\n",
    "x_train /= 255 \n",
    "x_test  /= 255\n",
    "\n",
    "\n",
    "print(\"Number of training samples in full dataset: \" + str(x_train.shape[0]))\n",
    "print(\"Number of testing samples in full dataset: \" + str(x_test.shape[0]))\n",
    "\n",
    "# Create a binary version for binary classification\n",
    "df = pd.DataFrame(np.vstack([x_train, x_test]))\n",
    "df['label'] = pd.Series(np.concatenate([y_train, y_test]))\n",
    "binary_data = df[df['label'].isin([0, 1])]\n",
    "\n",
    "\n",
    "binary_x_train = binary_data.iloc[:10000].drop(['label'], axis=1).as_matrix()\n",
    "binary_x_test  = binary_data.iloc[10000:].drop(['label'], axis=1).as_matrix()\n",
    "binary_y_train = binary_data.iloc[:10000]['label'].as_matrix()\n",
    "binary_y_test  = binary_data.iloc[10000:]['label'].as_matrix()\n",
    "\n",
    "print()\n",
    "print(\"Total number of binary 0 / 1 examples: \", binary_data.shape)\n",
    "print(\"Number of training samples in BINARY dataset: \" + str(binary_x_train.shape[0]))\n",
    "print(\"Number of testing samples in BINARY dataset: \" + str(binary_x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out what our data looks like\n",
    "\n",
    "Let's plot one of the training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = binary_x_train[0]\n",
    "img_rows, img_cols, channels = 28, 28, 1\n",
    "image = np.reshape(image, [img_rows, img_cols])\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "# The shape of each image is a vector with 784 binary values (\"pixels\")\n",
    "image_shape = binary_x_train[0].shape\n",
    "print(\"The shape of each image is\" + str(image_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make our model!!\n",
    "\n",
    "We're going to train a model that Keras calls a \"Sequential\" model.\n",
    "\n",
    "Sequential is an abstraction for really simple networks. There are many ways you can \"link\" the neurons between layers in a neural network, and \"Sequential\" is the simplest way to link these layers. Since we're training a single layer network, this doesn't really concern us.\n",
    "\n",
    "Sidenote: The alternative to in Keras to Sequential models are \"Functional\" models. You can use those to make fancier networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model = Sequential()\n",
    "num_classes = 1\n",
    "binary_model.add(Dense(num_classes, activation='sigmoid', input_shape=image_shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "\n",
    "Now that we've told the Keras *what the model is*, we now need to tell it *how to learn*.\n",
    "\n",
    "1. What's the loss function?\n",
    "\n",
    "    In class we've seen some loss functions like the hinge loss and the SVM loss. Here we're going to use a different loss function called the \"binary crossentropy\" loss. Cross-entropy is a loss function that works well for learning because it makes learning very fast when your function is \"very wrong\" but slower when it is pretty close to the true function.\n",
    "\n",
    "\n",
    "2. What's the optimizer?\n",
    "\n",
    "    We'll use SGD (Stochastic Gradient Descent) which we've already discussed in class.\n",
    "    \n",
    "\n",
    "3. Which metric to optimize?\n",
    "\n",
    "    We'll use accuracy- which is what we've been using for our algorithms all semester. There are some other options that make sense for other types of datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model.compile(loss='binary_crossentropy', optimizer=SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "history = binary_model.fit(binary_x_train, binary_y_train,\n",
    "                   batch_size=global_batch_size, # average 128 examples in each SGD test\n",
    "                   epochs=num_epochs,\n",
    "                   verbose=1,\n",
    "                   validation_data=(binary_x_test, binary_y_test)) #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out that accuracies!\n",
    "\n",
    "99% accuracy!\n",
    "\n",
    "Let's try it on our *test set* instead of our *validation set* now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = binary_model.evaluate(binary_x_test, binary_y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class classification!\n",
    "\n",
    "###  Format the labels\n",
    "\n",
    "The labels are currently formatted as numeric values, ie the image above has the label `5`.\n",
    "\n",
    "Since we're training 10 binary classifiers, we need to convert the labels into binary values.\n",
    "\n",
    "ie, the label `1` becomes the array `[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]`. The entry in the \"1th\" position is true, and all others are false. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifying 10 digits\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "print(\"shape of our labels before\" + str(y_train.shape))\n",
    "print(\"the first 3 labels look like:\") \n",
    "print(str(y_train[0:5]) + '\\n')\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test,  num_classes)\n",
    "\n",
    "print(\"shape of our labels after\" + str(y_train.shape))\n",
    "print(\"the first 3 labels look like:\") \n",
    "print(str(y_train[0:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is good to go, let's train our 10 classifiers!\n",
    "\n",
    "\n",
    "### Let's make our model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg_model = Sequential()\n",
    "logistic_reg_model.add(Dense(num_classes, activation='softmax', input_shape=image_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg_model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model & store accuracies!\n",
    "\n",
    "Note: turning on verbosity can slow things down *a lot* so if you're running some large model, turn it off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "history = logistic_reg_model.fit(x_train, y_train,\n",
    "                   batch_size=global_batch_size, # average 128 examples in each SGD test\n",
    "                   epochs=num_epochs,\n",
    "                   verbose=1,\n",
    "                   validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "score = logistic_reg_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Learning MNIST with a Multilayer Network!\n",
    "\n",
    "We'll set up a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = Sequential()\n",
    "mlp_model.add(Dense(512, activation='relu', input_shape=image_shape))\n",
    "mlp_model.add(Dense(256, activation='relu'))\n",
    "mlp_model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the MLP\n",
    "\n",
    "We'll compile with the same parameters as before so that it's a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "history = mlp_model.fit(x_train, y_train,\n",
    "                   batch_size=global_batch_size, # average 128 examples in each SGD test\n",
    "                   epochs=num_epochs,\n",
    "                   verbose=1,\n",
    "                   validation_data=(x_test, y_test))\n",
    "\n",
    "score = mlp_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# How deep can you go: can adding more layers help us?\n",
    "\n",
    "Creating a deep model gives us some performance gains, but it's not a panacea.  Let's see how model performance behaves as we add more layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_accuracy_analysis(x_train, y_train, x_test, y_test, max_layers, layer_size =128, verbose=False):\n",
    "    # type: (np.ndarray, np.ndarray, np.ndarray, np.ndarray, int, int, bool) -> List[float]\n",
    "    \"\"\"This method creates deep feedforward networks of varyig depth from 1 layer to max_layers.  It\n",
    "    returns a list of accuracies on the test set\n",
    "    \n",
    "    Args:\n",
    "        x_train: An np.ndarray of data where each row is a new data point\n",
    "        y_train: An np.ndarray of labels where each row is a binary vector with\n",
    "            1 in the column of the associated class\n",
    "        x_test: An np.ndarray of data where each row is a new data point\n",
    "        y_test: An np.ndarray of labels where each row is a binary vector with\n",
    "            1 in the column of the associated class\n",
    "        max_layers: An int the maximum depth to try\n",
    "        layer_size: The number of neurons in each layer\n",
    "    \n",
    "    Returns:\n",
    "        A List of floats as the accuracies of the model on the test set\n",
    "    \"\"\"\n",
    "    acc_scores = []\n",
    "    \n",
    "    # Build up and test 1 - max layers\n",
    "    for i in range(0, max_layers):\n",
    "        mlp_model = Sequential()\n",
    "        mlp_model.add(Dense(layer_size, activation='relu', input_shape=x_train[0].shape))\n",
    "        \n",
    "        # Add i layers\n",
    "        for j in range(1, i):\n",
    "            mlp_model.add(Dense(layer_size, activation='relu'))\n",
    "\n",
    "        mlp_model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "        mlp_model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n",
    "        history = mlp_model.fit(x_train, y_train,\n",
    "                        batch_size=128,\n",
    "                        epochs=10,\n",
    "                        verbose=0,\n",
    "                        validation_data=(x_test, y_test))\n",
    "        score = mlp_model.evaluate(x_test, y_test, verbose=0)\n",
    "        \n",
    "        # Print out scores if verbosity is non 0\n",
    "        if verbose:\n",
    "            print('Test loss:', score[0])\n",
    "            print('Test accuracy:', score[1])\n",
    "        acc_scores.append(score)\n",
    "    \n",
    "    return acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_layers = 30\n",
    "layer_size = 128\n",
    "scores = depth_accuracy_analysis(x_train, y_train, x_test, y_test, max_layers, layer_size=layer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, max_layers + 1), map(lambda x: x[1], scores), 'o-')\n",
    "plt.title('Behaviour of model accuracy with depth')\n",
    "plt.ylabel('Testing Accuracy')\n",
    "plt.xlabel('Number of layers with {} nodes/layer'.format(layer_size))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, max_layers + 1), map(lambda x: x[0], scores), 'o-')\n",
    "plt.title('Behaviour of model loss with depth')\n",
    "plt.ylabel('Testing loss')\n",
    "plt.xlabel('Number of layers with {} nodes/layer'.format(layer_size))\n",
    "plt.tight_layout()\n",
    "plt.savefig('depth_experiment_{}-layers_{}-layer_size.png'.format(max_layers, layer_size))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results:\n",
    "\n",
    "<img src=\"depth_experiment_30-layers_128-layer_size.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# If adding more layers isn't helping, can we get smarter about the network?\n",
    "\n",
    "There are many different types of layers that we can use when building a network.  Right now we have just seen dense layers which are simply perceptron-like nodes stacked together.  Here we will use a set of layers that are well adapted to images called convoluational, and pooling layers.  If you are interested in understanding how these work check out this great paper https://arxiv.org/pdf/1603.07285.pdf.  For now, you can think of the convlutional layers as learning filter like features of increasing complexity.  We will need to redefine our data in order to work with these layers.  Convolutions work on a matrix of values as opposed to a flattened array, because the filters take into consideration local patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "image_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=image_shape))\n",
    "cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(128, activation='relu'))\n",
    "cnn_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn_model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# State of the Art-ish Accuracy\n",
    "\n",
    "The state of the art on MNIST is 99.79% accuracy.  With a little extra work we can push our conv net pretty close to this at ~99.16% accuracy.  We can do this by adding regularization in the form of drop out and a better optimizer in the form of ADAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best CNN\n",
    "best_cnn_model = Sequential()\n",
    "best_cnn_model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                   activation='relu',\n",
    "                   input_shape=image_shape))\n",
    "best_cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "best_cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "best_cnn_model.add(Dropout(.5))\n",
    "best_cnn_model.add(Flatten())\n",
    "best_cnn_model.add(Dense(128, activation='relu'))\n",
    "best_cnn_model.add(Dropout(.5))\n",
    "best_cnn_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "best_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cnn_model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = best_cnn_model.fit(x_train, y_train,\n",
    "                             batch_size=128,\n",
    "                             epochs=10,\n",
    "                             verbose=1,\n",
    "                             validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = best_cnn_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Cool stuff with neural nets\n",
    "\n",
    "- Michael Nielsen's book is awesome: http://neuralnetworksanddeeplearning.com/\n",
    "\n",
    "- Lots of keras tutorials: https://github.com/fchollet/keras/tree/master/examples\n",
    "\n",
    "- \"Deep art\": https://deepart.io/\n",
    "\n",
    "- Adversarial Networks: https://blog.openai.com/adversarial-example-research/\n",
    "\n",
    "- NN visualization: http://playground.tensorflow.org/\n",
    "\n",
    "- Generated cat images from outlines: https://affinelayer.com/pixsrv/\n",
    "\n",
    "- AlphaGo: https://deepmind.com/research/alphago/\n",
    "\n",
    "- Music Generation: https://magenta.tensorflow.org/welcome-to-magenta\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
